{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43b2c9-ef6e-4679-a80d-d40fd01beb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GPU avaliable: \", torch.cuda.is_available())\n",
    "print(\"No. of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "print(\"Device index: \", torch.cuda.current_device())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31cad01-b24c-4ab4-ba3e-2610c2bc5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax\n",
    "\n",
    "#----img prepro func-----\n",
    "from torchvision.transforms import (\n",
    "    RandomCrop,\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomResizedCrop,\n",
    "    RandomApply,\n",
    "    RandomChoice,\n",
    "    RandomRotation,\n",
    "    Resize,\n",
    "    RandomErasing,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    ToPILImage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916fb1ed-2473-41a8-8b75-2c6733f8ecb4",
   "metadata": {},
   "source": [
    "# Set Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b5073a-6393-4a1e-8df1-ab8a939c5198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/convnextv2-atto-1k-224\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9590ee0-5dae-4224-8b19-401041726e31",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c337a6-1406-430c-a441-f4b56f859e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "roc_auc = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    predictions_prob = softmax(eval_pred.predictions, axis=1)[:,1]\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    precision_score = precision.compute(predictions=predictions, references=eval_pred.label_ids, average=None)\n",
    "    recall_score = recall.compute(predictions=predictions, references=eval_pred.label_ids, average=None)\n",
    "    f1_score = f1.compute(predictions=predictions, references=eval_pred.label_ids, average=None)\n",
    "    roc_auc_score = roc_auc.compute(prediction_scores=predictions_prob, references=eval_pred.label_ids)\n",
    "    \n",
    "    return {\"accuracy\": accuracy_score[\"accuracy\"],\n",
    "           \"precision_0\": precision_score[\"precision\"][0],\n",
    "           \"precision_1\": precision_score[\"precision\"][1],\n",
    "           \"recall_0\": recall_score[\"recall\"][0],\n",
    "            \"recall_1\": recall_score[\"recall\"][1],\n",
    "            \"f1_0\": f1_score[\"f1\"][0],\n",
    "           \"f1_1\": f1_score[\"f1\"][1],\n",
    "           \"roc_auc\": roc_auc_score[\"roc_auc\"]}\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d64a2-f486-4264-b80d-7df9743cb1bc",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cccb2ff-d0d3-4c5c-b772-8bcbb26f56d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----setup----\n",
    "labels = [\"REAL\", \"FAKE\"]\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "    \n",
    "def reformat_dataset(example):\n",
    "    example[\"label\"] = label2id[example[\"json\"][\"label\"].upper()]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb67fff-7d10-41bd-b853-027a451c8928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----load dataset----\n",
    "WEBDATASET_ROOT = \"data/webdataset/\"\n",
    "fnl = os.listdir(WEBDATASET_ROOT)\n",
    "fnl = [f for f in fnl if 'ipynb_checkpoints' not in f]\n",
    "\n",
    "train_real_data_l = {}\n",
    "train_fake_data_l = {}\n",
    "for dataset in fnl:\n",
    "    train_data = load_dataset(\"webdataset\", data_dir=WEBDATASET_ROOT+dataset, split=\"train\")\n",
    "    print(dataset, len(train_data))\n",
    "\n",
    "    train_data = train_data.map(reformat_dataset, num_proc=os.cpu_count())\n",
    "    train_real_data = train_data.filter(lambda x: x[\"label\"] == 0)\n",
    "    train_fake_data = train_data.filter(lambda x: x[\"label\"] == 1)\n",
    "    if dataset in ['ucf_selfie_dataset','covid_fmd_dataset','FMD_mask_data','mask_detection_data']:\n",
    "        train_real_data = train_real_data.rename_column(\"jpg\", \"image\")\n",
    "        train_fake_data = train_fake_data.rename_column(\"jpg\", \"image\")\n",
    "    else:\n",
    "        train_real_data = train_real_data.rename_column(\"png\", \"image\")\n",
    "        train_fake_data = train_fake_data.rename_column(\"png\", \"image\")\n",
    "    train_real_data = train_real_data.remove_columns([\"json\"])\n",
    "    train_fake_data = train_fake_data.remove_columns([\"json\"])\n",
    "    \n",
    "    train_real_data_l[dataset] = train_real_data\n",
    "    train_fake_data_l[dataset] = train_fake_data\n",
    "\n",
    "valid_real_data_l = {}\n",
    "valid_fake_data_l = {}\n",
    "for dataset in fnl:\n",
    "    valid_data = load_dataset(\"webdataset\", data_dir=WEBDATASET_ROOT+dataset, split=\"validation\")\n",
    "    print(dataset,len(valid_data))\n",
    "    \n",
    "    valid_data = valid_data.map(reformat_dataset, num_proc=os.cpu_count())\n",
    "    valid_real_data = valid_data.filter(lambda x: x[\"label\"] == 0)\n",
    "    valid_fake_data = valid_data.filter(lambda x: x[\"label\"] == 1)\n",
    "    if dataset in ['ucf_selfie_dataset','covid_fmd_dataset','FMD_mask_data','mask_detection_data']:\n",
    "        valid_real_data = valid_real_data.rename_column(\"jpg\", \"image\")\n",
    "        valid_fake_data = valid_fake_data.rename_column(\"jpg\", \"image\")\n",
    "    else:\n",
    "        valid_real_data = valid_real_data.rename_column(\"png\", \"image\")\n",
    "        valid_fake_data = valid_fake_data.rename_column(\"png\", \"image\")\n",
    "    valid_real_data = valid_real_data.remove_columns([\"json\"])\n",
    "    valid_fake_data = valid_fake_data.remove_columns([\"json\"])\n",
    "    \n",
    "    valid_real_data_l[dataset] = valid_real_data\n",
    "    valid_fake_data_l[dataset] = valid_fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8307706f-f4e9-41c2-a54e-ac18345f52e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c4986-76e9-4e3d-ac49-4b865a1c0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "train_ds = copy.deepcopy(final_train_dataset)\n",
    "val_ds = copy.deepcopy(final_valid_dataset)\n",
    "\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)\n",
    "print(f'train_ds={len(train_ds)}, val_ds={len(val_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771df4d-90c0-4d6e-aec6-d17b39c7b859",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f1e50-33c1-4398-81ae-695f658c1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----training---\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "784d7258-0d54-46ad-9c09-2d3ab61896b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_output}/{model_name}-{experiment_id}/{config.learning_rate}-{config.batch_size}-{config.epochs}\",\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps = 1000,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 1000,\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        num_train_epochs=config.epochs,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1000,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc\",\n",
    "        no_cuda=False,\n",
    "        dataloader_num_workers=8,\n",
    "        dataloader_prefetch_factor=2,\n",
    "        dataloader_pin_memory=True\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStoppingCallback(early_stopping_patience=5)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "        callbacks=[early_stop],\n",
    "        # optimizers=(optimizer, None)\n",
    "    )\n",
    "    \n",
    "    train_results = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m126"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
