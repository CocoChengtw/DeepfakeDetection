{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67fab3c",
   "metadata": {},
   "source": [
    "# t-SNE Analysis of Deepfake Detection Features\n",
    "\n",
    "This notebook demonstrates how t-SNE is applied to visualize high-dimensional feature embeddings extracted from deepfake detection models. The purpose is to analyze clustering behavior between real and fake samples before and after model tuning.\n",
    "\n",
    "This version is curated for external readers and reviewers (e.g., admissions committees) to showcase data analysis, function design, and visualization skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bde7cd",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data\n",
    "We begin by loading preprocessed features and labels extracted from our image classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5997d9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cad01-b24c-4ab4-ba3e-2610c2bc5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoModelForImageClassification, AutoModel, TrainingArguments, Trainer\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax\n",
    "\n",
    "#----img prepro func-----\n",
    "from torchvision.transforms import (\n",
    "    RandomCrop,\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomResizedCrop,\n",
    "    RandomApply,\n",
    "    RandomChoice,\n",
    "    RandomRotation,\n",
    "    Resize,\n",
    "    RandomErasing,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    ToPILImage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff387f-027d-4d01-b94e-1f9decaa7644",
   "metadata": {},
   "source": [
    "# Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdb204-e675-49c5-8b56-7ac4520bf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ee67c-b9f2-4434-84b8-598c0ebc74ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"REAL\", \"FAKE\"]\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "    \n",
    "def reformat_dataset(example):\n",
    "    example[\"label\"] = label2id[example[\"json\"][\"label\"].upper()]\n",
    "    return example\n",
    "\n",
    "#-----load dataset----\n",
    "WEBDATASET_ROOT = \"data/webdataset/\"\n",
    "fnl = os.listdir(WEBDATASET_ROOT)\n",
    "fnl = [f for f in fnl if 'ipynb_checkpoints' not in f]\n",
    "\n",
    "test_data_l = {}\n",
    "for dataset in fnl:\n",
    "    test_data = load_dataset(\"webdataset\", data_dir=WEBDATASET_ROOT+dataset, split=\"test\")\n",
    "    print(dataset, len(test_data))\n",
    "    \n",
    "    test_data = test_data.map(reformat_dataset, num_proc=os.cpu_count())\n",
    "    if dataset in ['ucf_selfie_dataset','covid_fmd_dataset','FMD_mask_data','mask_detection_data']:\n",
    "        test_data = test_data.rename_column(\"jpg\", \"image\")\n",
    "    elif dataset == 'testing':\n",
    "        test_data = test_data.rename_column(\"jpeg\", \"image\")\n",
    "    else:\n",
    "        test_data = test_data.rename_column(\"png\", \"image\")\n",
    "    test_data = test_data.remove_columns([\"json\"])\n",
    "    test_data_l[dataset] = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b80b1-0465-4b0b-a6cc-3edfaeb18651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_test_real_l = []\n",
    "ds_test_fake_l = []\n",
    "for name, ds in test_data_l.items():\n",
    "    ds_real = ds.filter(lambda x:x[\"label\"]==0, num_proc=os.cpu_count()) \n",
    "    if name in dataset_n_lst:\n",
    "        ds_fake = ds.filter(lambda x:x[\"label\"]==1, num_proc=os.cpu_count())\n",
    "        \n",
    "    if any(ds_real):\n",
    "        ds_test_real_l.append(ds_real)\n",
    "    if any(ds_fake):\n",
    "        ds_test_fake_l.append(ds_fake)\n",
    "\n",
    "    print(f'name={name}, tot={len(ds)}, real={len(ds_real)}, fake={len(ds_fake)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c504e-2251-4f24-be0d-2bd0a4419167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea9702-17b2-47d4-b064-5070ac3f4c6d",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a316a3-bb4b-4e58-8ee5-0dadd725b2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "print(size, crop_size)\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dbba0-e57a-4e54-aba8-715efa0240a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_image_processor = AutoImageProcessor.from_pretrained(checkpoint_path)\n",
    "saved_model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "saved_model = saved_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90060d9e",
   "metadata": {},
   "source": [
    "## Design Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad167f-3b5a-4450-ac72-504be00d6d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_all(df):\n",
    "    roc_auc_metric = evaluate.load(\"roc_auc\")\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    preds = df[\"pred\"].values\n",
    "    pred_probs = df[\"pred_prob\"].values\n",
    "    refs = df[\"label\"].values\n",
    "\n",
    "    acc = acc_metric.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "    roc_auc = roc_auc_metric.compute(prediction_scores=pred_probs, references=refs)[\"roc_auc\"]\n",
    "    precision= precision_metric.compute(predictions=preds, references=refs, average=None)\n",
    "    recall = recall_metric.compute(predictions=preds, references=refs, average=None)\n",
    "    f1 = f1_metric.compute(predictions=preds, references=refs, average=None)\n",
    "\n",
    "    result = {\"accuracy\": acc,\n",
    "              \"AUC\": roc_auc,\n",
    "              \"precision\": precision,\n",
    "              \"recall\": recall,\n",
    "              \"f1\": f1}\n",
    "    return result\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef2f65-816e-4fe4-9ec8-d9f1fa055696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_inference_result(model, test_data_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    pred_prob_list = []\n",
    "\n",
    "\n",
    "    for step, batch in enumerate(tqdm(test_data_loader)):\n",
    "\n",
    "        batch = {\"pixel_values\": batch[\"pixel_values\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device)}\n",
    "        with torch.no_grad():\n",
    "            outputs = saved_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        pred_probs = outputs.logits.softmax(dim=-1)[:,1]\n",
    "        references = batch[\"labels\"]\n",
    "        pred_list += list(predictions)\n",
    "        pred_prob_list+= list(pred_probs)\n",
    "\n",
    "    return (pred_list, pred_prob_list)\n",
    "\n",
    "def format_evaluate_result_df(result):\n",
    "    new_dic = {}\n",
    "    new_dic[\"Accuracy\"] = round(result[\"accuracy\"],4)\n",
    "    \n",
    "    if \"AUC\" in result:\n",
    "        new_dic[\"AUC\"] = round(result[\"AUC\"],4)\n",
    "    new_dic[\"Precision 0\"] = round(result[\"precision\"][\"precision\"][0], 4)\n",
    "    new_dic[\"Precision 1\"] = round(result[\"precision\"][\"precision\"][1], 4)\n",
    "    new_dic[\"Recall 0\"] = round(result[\"recall\"][\"recall\"][0], 4)\n",
    "    new_dic[\"Recall 1\"] = round(result[\"recall\"][\"recall\"][1], 4)\n",
    "    new_dic[\"F1 0\"] = round(result[\"f1\"][\"f1\"][0], 4)\n",
    "    new_dic[\"F1 1\"] = round(result[\"f1\"][\"f1\"][1], 4)\n",
    "    \n",
    "    result_df = pd.DataFrame([new_dic])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba69aa",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59202ba-ff14-4a06-8ea0-40c67761065c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Evaluation dataloader\n",
    "eval_batch_size = 1024\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=eval_batch_size, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef506d33-fd92-4966-a30b-1c35989643ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_result = get_inference_result(saved_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a0847-9efc-4850-bbc2-49885f4e2eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model_embedding = AutoModel.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "saved_model_embedding = saved_model_embedding.to(device)\n",
    "\n",
    "def detect_deepfake_embeddings(model, test_data_loader):\n",
    "        \n",
    "    model.eval()\n",
    "    pred_embedding_list = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(test_data_loader)):\n",
    "        # print(batch)\n",
    "        # print(step)\n",
    "        batch = {\"pixel_values\": batch[\"pixel_values\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device)}\n",
    "        batch.pop('labels')\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = model(**batch).last_hidden_state\n",
    "            # hidden_output = torch.cat(last_hidden_state, dim=0)\n",
    "        split_tensors = torch.split(last_hidden_state, 1, dim=0)\n",
    "        pred_embedding_list += list(split_tensors)\n",
    "        \n",
    "    return pred_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f9213-d9c1-4877-8209-85b92a68b5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_embedding = detect_deepfake_embeddings(saved_model_embedding, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5a365-f25f-4f47-8a20-e52be629585f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_embedding = [tensor.cpu().numpy() for tensor in inf_embedding]\n",
    "test_embedding = {\"key\": test_df['key'], \"embeddings\": list(inf_embedding)}\n",
    "test_embedding_df = pd.DataFrame(test_embedding)\n",
    "test_df = test_df.merge(test_embedding_df, how='left', on='key')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4193b",
   "metadata": {},
   "source": [
    "## Design Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe99ad3-fcce-4326-a084-a7b035e261e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275d386-2cac-45c8-b8b6-54a3eceb39e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_flatten_data(df, dataset_list, target_column, group, error=False, error_ind=None):\n",
    "    \"\"\"Flatten data into 1-dimensional data\"\"\"\n",
    "    df = df[df['dataset'].isin(dataset_list)]\n",
    "    processed_data = df[target_column].apply(lambda x: x.reshape(-1)).to_numpy()\n",
    "    processed_data = np.vstack(processed_data).astype(np.float32)\n",
    "    labels = df['label'].to_list()\n",
    "    sources = df[group].to_list()\n",
    "        \n",
    "    return processed_data, labels, sources\n",
    "\n",
    "\n",
    "def apply_dimensionality_reduction(data, method='tsne', n_components=2, **kwargs):\n",
    "    \"\"\"\n",
    "    :param data: numpy array, shape (n_samples, n_features)\n",
    "    :param method: 'tsne' or 'pca'\n",
    "    :param n_components: number of dimensions to reduce to\n",
    "    :return: numpy array, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    if method == 'tsne':\n",
    "        return TSNE(n_components=n_components, random_state=42, **kwargs).fit_transform(data)\n",
    "    elif method == 'pca':\n",
    "        return PCA(n_components=n_components, random_state=42, **kwargs).fit_transform(data)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'tsne' or 'pca'\")\n",
    "      \n",
    "    \n",
    "def visualize_plotly_custom(X, labels, sources, title):\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'x': X[:, 0],\n",
    "        'y': X[:, 1],\n",
    "        'label': labels,\n",
    "        'source': sources\n",
    "    })\n",
    "    df['label'] = df['label'].replace({0: 'real', 1: 'fake'})\n",
    "\n",
    "    color_map = {\n",
    "        'fake': 'rgba(244, 97, 151, 0.5)',\n",
    "        'real': 'rgba(65, 157, 120, 0.5)'\n",
    "    }\n",
    "\n",
    "    marker_map = {\n",
    "        source: marker for source, marker in zip(\n",
    "            df['source'].unique(),\n",
    "            ['circle', 'square', 'diamond', 'cross', 'x', 'triangle-up', 'triangle-down', 'triangle-left', 'triangle-right', 'pentagon', \n",
    "             'hexagram', 'star', 'bowtie', 'star-diamond', 'diamond-tall', 'diamond-wide', 'hourglass',\n",
    "             'circle-open', 'square-open', 'diamond-open', 'cross-open', 'x-open', \n",
    "             'triangle-up-open', 'triangle-down-open', 'triangle-left-open', 'triangle-right-open', 'pentagon-open', \n",
    "             'hexagram-open', 'star-open', 'bowtie-open', 'star-diamond-open', 'diamond-tall-open', \n",
    "             'diamond-wide-open', 'hourglass-open']\n",
    "        )\n",
    "    }\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for label in ['fake', 'real']:\n",
    "        for source in list(set(sources)):\n",
    "            subset = df[(df['label'] == label) & (df['source'] == source)]\n",
    "            fig.add_trace(go.Scattergl(\n",
    "                x=subset['x'],\n",
    "                y=subset['y'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=color_map[label],\n",
    "                    size=10,\n",
    "                    symbol=marker_map[source],\n",
    "                    # line=dict(width=0.3, color='DarkSlateGrey')\n",
    "                ),\n",
    "                name=f'{label}-{source}',\n",
    "                hovertext=f'{label}-{source}',\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "            \n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='t-SNE 1',\n",
    "        yaxis_title='t-SNE 2',\n",
    "        legend_title='Label-Source',\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        plot_bgcolor='rgb(240,240,240)',\n",
    "        hovermode='closest'\n",
    "    )\n",
    "\n",
    "    fig.write_html(f\"visualization/{title}.html\")\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe120cb",
   "metadata": {},
   "source": [
    "## Step 2: t-SNE Visualization - Before Tuning\n",
    "We apply t-SNE to visualize the embeddings of the model before tuning. This helps identify overlapping areas between real and fake samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9bc71-a6af-4563-bca9-0eb69795c7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = test_df['dataset'].unique().tolist()\n",
    "target_column = 'embeddings'\n",
    "group = 'dataset'\n",
    "reduced_method = 'tsne'\n",
    "detail_info = 'v4_model_testing_dataset'\n",
    "\n",
    "raw_processed_data, raw_labels, raw_sources = get_flatten_data(test_df, dataset_list, target_column, group)\n",
    "raw_reduced_data = apply_dimensionality_reduction(raw_processed_data, reduced_method)\n",
    "visualize_plotly_custom(raw_reduced_data, raw_labels, raw_sources, f'{reduced_method} visualization of {detail_info} focusing on raw signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2aa8b6",
   "metadata": {},
   "source": [
    "## Step 3: t-SNE Visualization - After Tuning\n",
    "After threshold optimization and fine-tuning, the separation between clusters becomes clearer. This step visualizes the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25474a9c-183d-44df-9f23-5ea070137c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = test_df['dataset'].unique().tolist()\n",
    "target_column = 'embeddings'\n",
    "group = 'dataset'\n",
    "reduced_method = 'tsne'\n",
    "detail_info = 'v5_model_testing_dataset'\n",
    "\n",
    "raw_processed_data, raw_labels, raw_sources = get_flatten_data(test_df, dataset_list, target_column, group)\n",
    "raw_reduced_data = apply_dimensionality_reduction(raw_processed_data, reduced_method)\n",
    "visualize_plotly_custom(raw_reduced_data, raw_labels, raw_sources, f'{reduced_method} visualization of {detail_info} focusing on raw signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ad980",
   "metadata": {},
   "source": [
    "## Summary\n",
    "t-SNE analysis enabled us to visually diagnose model misclassifications and improve overall performance by fine-tuning thresholds and separating ambiguous samples."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
